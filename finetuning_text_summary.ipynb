{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUoFKfO_hsdW"
      },
      "source": [
        "**Install packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNt2EwG4bRi5"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/huggingface/peft.git\n",
        "!pip install bitsandbytes==0.35.0\n",
        "!pip install transformers==4.31\n",
        "!pip install -q datasets\n",
        "!pip install -qqq trl==0.7.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVpqLugtbdMS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZFR3_a7iDV5"
      },
      "source": [
        "**Dataset = CNN Daily Mail**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl_1O1nWbYNq"
      },
      "outputs": [],
      "source": [
        "huggingface_dataset_name = \"cnn_dailymail\"\n",
        "dataset = load_dataset(huggingface_dataset_name, \"3.0.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5OCq_pCbi-X"
      },
      "outputs": [],
      "source": [
        "sample = dataset[\"train\"][1]\n",
        "print(sample[\"article\"][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYbLOUCrfzjD"
      },
      "source": [
        "**Prompt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnbKYmwZbp9A"
      },
      "source": [
        "```\n",
        "    Summarize the following conversation.\n",
        "    \n",
        "    ### Input:\n",
        "    (CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third\n",
        "    gold in Moscow as he anchored Jamaica to\n",
        "    victory in the men's 4x100m relay. The fastest man in the world charged clear of United\n",
        "    States rival Justin Gatlin as the Jamaican\n",
        "    quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36\n",
        "    seconds. The U.S finished second in 37.56 seconds\n",
        "    with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
        "    The 26-year-old Bolt has n......\n",
        "    \n",
        "\n",
        "    ### Summary:\n",
        "\n",
        "    Usain Bolt wins third gold of world championship .\n",
        "    Anchors Jamaica to 4x100m relay victory .\n",
        "    Eighth gold at the championships for Bolt .\n",
        "    Jamaica double up in women's 4x100m relay .\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApGVN5AOjrx7"
      },
      "source": [
        "**Formatting the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLIbfGy4bsPP"
      },
      "outputs": [],
      "source": [
        "def format_instruction(dialogue: str, summary: str):\n",
        "    return f\"\"\"### Instruction:\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input:\n",
        "{dialogue.strip()}\n",
        "\n",
        "### Summary:\n",
        "{summary}\n",
        "\"\"\".strip()\n",
        "\n",
        "def generate_instruction_dataset(data_point):\n",
        "\n",
        "    return {\n",
        "        \"article\": data_point[\"article\"],\n",
        "        \"highlights\": data_point[\"highlights\"],\n",
        "        \"text\": format_instruction(data_point[\"article\"],data_point[\"highlights\"])\n",
        "    }\n",
        "\n",
        "def process_dataset(data: Dataset):\n",
        "    return (\n",
        "        data.shuffle(seed=42)\n",
        "        .map(generate_instruction_dataset).remove_columns(['id'])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNbsd623iiWp"
      },
      "source": [
        "**Shuffling the data and selecting some**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FklmTWPCbvV1"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
        "dataset[\"test\"] = process_dataset(dataset[\"validation\"])\n",
        "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
        "\n",
        "# 1000 rows\n",
        "train_data = dataset['train'].shuffle(seed=42).select([i for i in range(1000)])\n",
        "\n",
        "# 100 rows\n",
        "test_data = dataset['test'].shuffle(seed=42).select([i for i in range(100)])\n",
        "validation_data = dataset['validation'].shuffle(seed=42).select([i for i in range(100)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jsne_dJipqB"
      },
      "source": [
        "**Set up a language model with LLAMA-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF3b7-9jbxpl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id =  \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUVu8bipkrVX"
      },
      "source": [
        "**Summary with the model above**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNXouxElb0cu"
      },
      "outputs": [],
      "source": [
        "index = 2\n",
        "\n",
        "dialogue = test_data['article'][index]\n",
        "summary = test_data['highlights'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input:\n",
        "{dialogue}\n",
        "\n",
        "### Summary:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=100,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(f'MODEL GENERATION:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fonux0-JixJu"
      },
      "source": [
        "#Fine Tuning#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhyXzw0elGpN"
      },
      "source": [
        "**Knowledge distillation training (training a smaller model (the student model) from a teacher model**\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ILlUkAfb2-W"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_tp9XUrlx14"
      },
      "source": [
        "**Configure a model with Lora attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eK5ORJcb573"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NQRttJamBKA"
      },
      "source": [
        "**Configure various aspects of the fine-tuning process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIioMvbxb8Fv"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "OUTPUT_DIR = \"llama2-docsum-adapter\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    num_train_epochs=4,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    warmup_ratio=0.05,\n",
        "    save_strategy=\"epoch\",\n",
        "    group_by_length=True,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_safetensors=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        ")\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxszb6TQmNyW"
      },
      "source": [
        "**Train a model using sequence-level fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKIKgYstb-bk"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=validation_data,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljUJTcDgmZQ9"
      },
      "source": [
        "**Save the fine-tuned model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20SMfDzjcCa8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "peft_model_path = \"./peft-dialogue-summary\"\n",
        "\n",
        "if not os.path.exists(peft_model_path):\n",
        "    os.makedirs(peft_model_path)\n",
        "\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAjLjy8cmpEk"
      },
      "source": [
        "**Prepare the model for inference by using cache**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SAMuul1cEdk"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JThibYiSi4pH"
      },
      "source": [
        "**Hugging face Token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IcB2pBLhDcr"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1t6k3Vum5ry"
      },
      "source": [
        "**Load the trained model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7Jd779CcGMn"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "peft_model_dir = \"peft-dialogue-summary\"\n",
        "trained_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUzc3R36nF2Z"
      },
      "source": [
        "**Generate a summary using a trained PEFT model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie4Y5zILcKW-"
      },
      "outputs": [],
      "source": [
        "index = 51\n",
        "\n",
        "dialogue = train_data['article'][index][:10000]\n",
        "summary = train_data['highlights'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input:\n",
        "{dialogue}\n",
        "\n",
        "### Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors='pt',truncation=True).input_ids.cuda()\n",
        "outputs = trained_model.generate(input_ids=input_ids, max_new_tokens=200, )\n",
        "output= tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(f'TRAINED MODEL GENERATED TEXT :\\n{output}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
